#!/usr/bin/env python3
"""
RAG System - Retrieval Augmented Generation
=============================================
H·ªá th·ªëng RAG c·ª•c b·ªô cho miniZ MCP v4.3.0

Features:
1. DuckDuckGo Search - T√¨m ki·∫øm th√¥ng tin m·ªõi nh·∫•t t·ª´ Internet
2. Local Knowledge Base - T√†i li·ªáu n·ªôi b·ªô v·ªõi TF-IDF ranking
3. Hybrid RAG - K·∫øt h·ª£p c·∫£ 2 ngu·ªìn th√¥ng tin
4. Smart Context Builder - X√¢y d·ª±ng context th√¥ng minh cho LLM
5. Crypto API - Real-time prices from CoinGecko/Binance

Copyright ¬© 2025 miniZ Team
"""

import asyncio
import json
import os
import re
import hashlib
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import Counter
import math

# Crypto API
try:
    from crypto_api import get_crypto_price, get_crypto_price_binance, detect_crypto_query
    CRYPTO_API_AVAILABLE = True
except ImportError:
    CRYPTO_API_AVAILABLE = False
    print("‚ö†Ô∏è [CryptoAPI] Module not available")

# ============================================================
# CONFIGURATION
# ============================================================

RAG_CONFIG_FILE = Path(__file__).parent / "rag_config.json"
RAG_CACHE_FILE = Path(__file__).parent / "rag_cache.json"

DEFAULT_RAG_CONFIG = {
    "web_search": {
        "enabled": True,
        "max_results": 5,
        "cache_ttl_minutes": 30,
        "timeout_seconds": 10,
        "region": "vn-vi",  # Vietnam Vietnamese
        "safe_search": "moderate"
    },
    "knowledge_base": {
        "enabled": True,
        "folder_path": "",
        "max_results": 5,
        "chunk_size": 500,
        "chunk_overlap": 100
    },
    "hybrid": {
        "web_weight": 0.4,
        "local_weight": 0.6,
        "rerank_enabled": True
    }
}

# ============================================================
# DATA CLASSES
# ============================================================

@dataclass
class SearchResult:
    """K·∫øt qu·∫£ t√¨m ki·∫øm"""
    title: str
    snippet: str
    url: str = ""
    source: str = "web"  # "web" or "local"
    score: float = 0.0
    timestamp: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class RAGContext:
    """Context ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng cho LLM"""
    query: str
    web_results: List[SearchResult] = field(default_factory=list)
    local_results: List[SearchResult] = field(default_factory=list)
    combined_context: str = ""
    sources: List[str] = field(default_factory=list)
    search_time_ms: float = 0.0
    timestamp: str = ""

# ============================================================
# DUCKDUCKGO SEARCH MODULE
# ============================================================

class DuckDuckGoSearch:
    """
    DuckDuckGo Search - T√¨m ki·∫øm th√¥ng tin m·ªõi nh·∫•t t·ª´ Internet
    S·ª≠ d·ª•ng DuckDuckGo Instant Answer API v√† HTML scraping
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or DEFAULT_RAG_CONFIG["web_search"]
        self.cache = {}
        self.cache_ttl = self.config.get("cache_ttl_minutes", 30) * 60
        self._load_cache()
    
    def _load_cache(self):
        """Load cache t·ª´ file"""
        try:
            if RAG_CACHE_FILE.exists():
                with open(RAG_CACHE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.cache = data.get("web_cache", {})
        except Exception:
            self.cache = {}
    
    def _save_cache(self):
        """L∆∞u cache v√†o file"""
        try:
            cache_data = {"web_cache": self.cache, "last_update": datetime.now().isoformat()}
            with open(RAG_CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception:
            pass
    
    def _get_cache_key(self, query: str) -> str:
        """T·∫°o cache key t·ª´ query"""
        return hashlib.md5(query.lower().strip().encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict) -> bool:
        """Ki·ªÉm tra cache c√≤n h·ª£p l·ªá kh√¥ng"""
        if not cache_entry:
            return False
        cached_time = cache_entry.get("timestamp", 0)
        return (time.time() - cached_time) < self.cache_ttl
    
    async def search(self, query: str, max_results: int = None) -> List[SearchResult]:
        """
        T√¨m ki·∫øm tr√™n DuckDuckGo
        
        Args:
            query: T·ª´ kh√≥a t√¨m ki·∫øm
            max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
            
        Returns:
            List[SearchResult]: Danh s√°ch k·∫øt qu·∫£
        """
        if not self.config.get("enabled", True):
            return []
        
        max_results = max_results or self.config.get("max_results", 5)
        
        # Check cache
        cache_key = self._get_cache_key(query)
        if cache_key in self.cache and self._is_cache_valid(self.cache[cache_key]):
            cached = self.cache[cache_key]
            return [SearchResult(**r) for r in cached.get("results", [])]
        
        # Th·ª±c hi·ªán t√¨m ki·∫øm
        results = await self._perform_search(query, max_results)
        
        # Cache k·∫øt qu·∫£
        self.cache[cache_key] = {
            "timestamp": time.time(),
            "query": query,
            "results": [
                {
                    "title": r.title,
                    "snippet": r.snippet,
                    "url": r.url,
                    "source": r.source,
                    "score": r.score
                }
                for r in results
            ]
        }
        self._save_cache()
        
        return results
    
    async def _perform_search(self, query: str, max_results: int) -> List[SearchResult]:
        """Th·ª±c hi·ªán t√¨m ki·∫øm th·ª±c t·∫ø - Multi-provider v·ªõi fallback"""
        results = []
        
        # 1. Th·ª≠ Serper.dev API tr∆∞·ªõc (Google Search - ch√≠nh x√°c nh·∫•t)
        results = await self._search_with_serper(query, max_results)
        if results:
            print(f"‚úÖ [RAG] Serper search: {len(results)} results")
            return results
        
        # 2. Th·ª≠ DuckDuckGo library
        try:
            try:
                from ddgs import DDGS
            except ImportError:
                from duckduckgo_search import DDGS
            
            with DDGS() as ddgs:
                search_results = list(ddgs.text(
                    query,
                    region=self.config.get("region", "vn-vi"),
                    safesearch=self.config.get("safe_search", "moderate"),
                    max_results=max_results
                ))
                
                for i, r in enumerate(search_results):
                    results.append(SearchResult(
                        title=r.get("title", ""),
                        snippet=r.get("body", ""),
                        url=r.get("href", ""),
                        source="web",
                        score=1.0 - (i * 0.1),  # Score gi·∫£m d·∫ßn
                        timestamp=datetime.now().isoformat(),
                        metadata={"rank": i + 1, "provider": "duckduckgo"}
                    ))
                
                if results:
                    print(f"‚úÖ [RAG] DuckDuckGo search: {len(results)} results")
                    return results
                    
        except ImportError:
            print("‚ö†Ô∏è [RAG] DuckDuckGo library not installed")
        except Exception as e:
            print(f"‚ö†Ô∏è [RAG] DuckDuckGo search error: {e}")
        
        # 3. Fallback to HTML scraping
        results = await self._fallback_search(query, max_results)
        if results:
            print(f"‚úÖ [RAG] Fallback search: {len(results)} results")
        
        return results
    
    async def _search_with_serper(self, query: str, max_results: int) -> List[SearchResult]:
        """
        T√¨m ki·∫øm v·ªõi Serper.dev API (Google Search)
        Mi·ªÖn ph√≠ 2500 queries/th√°ng
        API Key: ƒê·∫∑t trong bi·∫øn m√¥i tr∆∞·ªùng SERPER_API_KEY ho·∫∑c rag_config.json
        """
        try:
            import requests
            
            # L·∫•y API key t·ª´ config ho·∫∑c env
            api_key = os.environ.get("SERPER_API_KEY", "")
            if not api_key:
                # Th·ª≠ ƒë·ªçc t·ª´ config
                try:
                    config_file = Path(__file__).parent / "rag_config.json"
                    if config_file.exists():
                        with open(config_file, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                            api_key = config.get("serper_api_key", "")
                except:
                    pass
            
            if not api_key:
                return []
            
            url = "https://google.serper.dev/search"
            headers = {
                "X-API-KEY": api_key,
                "Content-Type": "application/json"
            }
            payload = {
                "q": query,
                "gl": "vn",  # Vietnam
                "hl": "vi",  # Vietnamese
                "num": max_results
            }
            
            response = requests.post(url, headers=headers, json=payload, timeout=10)
            
            if response.status_code != 200:
                return []
            
            data = response.json()
            results = []
            
            # Parse organic results
            organic = data.get("organic", [])
            for i, item in enumerate(organic[:max_results]):
                results.append(SearchResult(
                    title=item.get("title", ""),
                    snippet=item.get("snippet", ""),
                    url=item.get("link", ""),
                    source="web",
                    score=1.0 - (i * 0.1),
                    timestamp=datetime.now().isoformat(),
                    metadata={"rank": i + 1, "provider": "serper_google"}
                ))
            
            # Also check knowledge graph for quick answers
            knowledge_graph = data.get("knowledgeGraph", {})
            if knowledge_graph:
                title = knowledge_graph.get("title", "")
                description = knowledge_graph.get("description", "")
                if title and description:
                    results.insert(0, SearchResult(
                        title=f"[Knowledge] {title}",
                        snippet=description,
                        url=knowledge_graph.get("website", ""),
                        source="web",
                        score=1.0,
                        timestamp=datetime.now().isoformat(),
                        metadata={"provider": "serper_knowledge_graph"}
                    ))
            
            # Check answer box
            answer_box = data.get("answerBox", {})
            if answer_box:
                answer = answer_box.get("answer", "") or answer_box.get("snippet", "")
                if answer:
                    results.insert(0, SearchResult(
                        title="[Direct Answer]",
                        snippet=answer,
                        url=answer_box.get("link", ""),
                        source="web",
                        score=1.0,
                        timestamp=datetime.now().isoformat(),
                        metadata={"provider": "serper_answer_box"}
                    ))
            
            return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è [RAG] Serper API error: {e}")
            return []
    
    async def _fallback_search(self, query: str, max_results: int) -> List[SearchResult]:
        """Fallback search s·ª≠ d·ª•ng DuckDuckGo HTML API + Google Lite"""
        results = []
        
        # Try DuckDuckGo HTML first
        try:
            import requests
            from urllib.parse import quote_plus
            
            url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "vi-VN,vi;q=0.9,en;q=0.8"
            }
            
            timeout = self.config.get("timeout_seconds", 15)
            response = requests.get(url, headers=headers, timeout=timeout)
            
            if response.status_code == 200:
                results = self._parse_ddg_html(response.text, max_results)
                if results:
                    return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è [RAG] DuckDuckGo HTML error: {e}")
        
        # Fallback to Google Lite (mobile version - simpler HTML)
        try:
            import requests
            from urllib.parse import quote_plus
            
            url = f"https://www.google.com/search?q={quote_plus(query)}&hl=vi&gl=vn"
            headers = {
                "User-Agent": "Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
                "Accept-Language": "vi-VN,vi;q=0.9"
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                # Parse Google results (simpler)
                html = response.text
                
                # T√¨m c√°c div ch·ª©a k·∫øt qu·∫£
                pattern = r'<div class="[^"]*">([^<]{50,500})</div>'
                snippets = re.findall(pattern, html)
                
                # L·ªçc c√°c snippet c√≥ √Ω nghƒ©a
                for i, snippet in enumerate(snippets[:max_results]):
                    # Clean HTML
                    clean = re.sub(r'<[^>]+>', '', snippet).strip()
                    if len(clean) > 30 and query.lower().split()[0] in clean.lower():
                        results.append(SearchResult(
                            title=f"K·∫øt qu·∫£ {i+1}",
                            snippet=clean[:300],
                            url="",
                            source="web",
                            score=1.0 - (i * 0.1),
                            timestamp=datetime.now().isoformat(),
                            metadata={"provider": "google_lite", "rank": i + 1}
                        ))
                        
        except Exception as e:
            print(f"‚ö†Ô∏è [RAG] Google Lite error: {e}")
        
        return results
    
    def _parse_ddg_html(self, html: str, max_results: int) -> List[SearchResult]:
        """Parse k·∫øt qu·∫£ t·ª´ DuckDuckGo HTML"""
        results = []
        
        # Simple regex parsing
        # Pattern cho title v√† link
        title_pattern = r'class="result__a"[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
        snippet_pattern = r'class="result__snippet"[^>]*>([^<]*)</span>'
        
        titles = re.findall(title_pattern, html)
        snippets = re.findall(snippet_pattern, html)
        
        for i, (url, title) in enumerate(titles[:max_results]):
            snippet = snippets[i] if i < len(snippets) else ""
            
            # Clean up
            title = re.sub(r'<[^>]+>', '', title).strip()
            snippet = re.sub(r'<[^>]+>', '', snippet).strip()
            
            if title:
                results.append(SearchResult(
                    title=title,
                    snippet=snippet,
                    url=url,
                    source="web",
                    score=1.0 - (i * 0.1),
                    timestamp=datetime.now().isoformat(),
                    metadata={"rank": i + 1}
                ))
        
        return results

# ============================================================
# LOCAL KNOWLEDGE BASE MODULE (Enhanced)
# ============================================================

class LocalKnowledgeBase:
    """
    Knowledge Base n·ªôi b·ªô v·ªõi TF-IDF ranking
    H·ªó tr·ª£: TXT, PDF, DOCX, MD, JSON
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or DEFAULT_RAG_CONFIG["knowledge_base"]
        self.index = {}
        self.documents = {}
        self.idf_cache = {}
        self._load_index()
    
    def _load_index(self):
        """Load index t·ª´ file"""
        try:
            index_file = Path(__file__).parent / "knowledge_index.json"
            if index_file.exists():
                with open(index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.index = data.get("index", {})
                    self.documents = data.get("documents", {})
        except Exception:
            pass
    
    def _tokenize(self, text: str) -> List[str]:
        """Tokenize vƒÉn b·∫£n th√†nh t·ª´"""
        # Lowercase v√† lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát
        text = text.lower()
        # Gi·ªØ l·∫°i ti·∫øng Vi·ªát v√† ti·∫øng Anh
        words = re.findall(r'[a-zA-Z√Ä-·ªπ]+', text)
        # Lo·∫°i b·ªè stopwords c∆° b·∫£n
        stopwords = {'v√†', 'c·ªßa', 'l√†', 'c√≥', 'trong', 'cho', 'ƒë∆∞·ª£c', 'v·ªõi', 'n√†y', 'ƒë√≥',
                     'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                     'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                     'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
                     'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as'}
        return [w for w in words if len(w) > 1 and w not in stopwords]
    
    def _calculate_tf(self, term: str, document: List[str]) -> float:
        """T√≠nh Term Frequency"""
        if not document:
            return 0.0
        return document.count(term) / len(document)
    
    def _calculate_idf(self, term: str) -> float:
        """T√≠nh Inverse Document Frequency"""
        if term in self.idf_cache:
            return self.idf_cache[term]
        
        total_docs = len(self.documents)
        if total_docs == 0:
            return 0.0
        
        docs_with_term = sum(1 for doc_id, doc in self.documents.items()
                           if term in self._tokenize(doc.get("content", "")))
        
        idf = math.log((total_docs + 1) / (docs_with_term + 1)) + 1
        self.idf_cache[term] = idf
        return idf
    
    def _calculate_tfidf_score(self, query: str, document: str) -> float:
        """T√≠nh TF-IDF score cho document v·ªõi query"""
        query_terms = self._tokenize(query)
        doc_terms = self._tokenize(document)
        
        if not query_terms or not doc_terms:
            return 0.0
        
        score = 0.0
        for term in query_terms:
            tf = self._calculate_tf(term, doc_terms)
            idf = self._calculate_idf(term)
            score += tf * idf
        
        return score
    
    def _extract_relevant_chunks(self, content: str, query: str, 
                                  chunk_size: int = 500, 
                                  chunk_overlap: int = 100) -> List[Tuple[str, float]]:
        """
        Tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn li√™n quan nh·∫•t
        S·ª≠ d·ª•ng sliding window approach
        """
        if not content or not query:
            return []
        
        words = content.split()
        if len(words) <= chunk_size:
            score = self._calculate_tfidf_score(query, content)
            return [(content, score)]
        
        chunks = []
        step = chunk_size - chunk_overlap
        
        for i in range(0, len(words) - chunk_size + 1, step):
            chunk = ' '.join(words[i:i + chunk_size])
            score = self._calculate_tfidf_score(query, chunk)
            chunks.append((chunk, score))
        
        # S·∫Øp x·∫øp theo score gi·∫£m d·∫ßn
        chunks.sort(key=lambda x: x[1], reverse=True)
        return chunks
    
    async def search(self, query: str, max_results: int = None) -> List[SearchResult]:
        """
        T√¨m ki·∫øm trong Knowledge Base
        
        Args:
            query: T·ª´ kh√≥a t√¨m ki·∫øm
            max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
            
        Returns:
            List[SearchResult]: Danh s√°ch k·∫øt qu·∫£
        """
        if not self.config.get("enabled", True):
            return []
        
        max_results = max_results or self.config.get("max_results", 5)
        chunk_size = self.config.get("chunk_size", 500)
        chunk_overlap = self.config.get("chunk_overlap", 100)
        
        results = []
        
        for doc_id, doc in self.documents.items():
            content = doc.get("content", "")
            title = doc.get("title", doc.get("filename", "Unknown"))
            file_path = doc.get("path", "")
            
            # Tr√≠ch xu·∫•t chunks li√™n quan
            chunks = self._extract_relevant_chunks(content, query, chunk_size, chunk_overlap)
            
            if chunks:
                best_chunk, score = chunks[0]
                
                if score > 0:
                    results.append(SearchResult(
                        title=title,
                        snippet=best_chunk[:500] + "..." if len(best_chunk) > 500 else best_chunk,
                        url=f"file://{file_path}",
                        source="local",
                        score=score,
                        timestamp=datetime.now().isoformat(),
                        metadata={
                            "doc_id": doc_id,
                            "file_path": file_path,
                            "total_chunks": len(chunks)
                        }
                    ))
        
        # S·∫Øp x·∫øp theo score
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:max_results]
    
    async def get_full_context(self, query: str, max_chars: int = 10000) -> str:
        """
        L·∫•y context ƒë·∫ßy ƒë·ªß t·ª´ c√°c documents li√™n quan
        
        Args:
            query: C√¢u h·ªèi
            max_chars: S·ªë k√Ω t·ª± t·ªëi ƒëa
            
        Returns:
            str: Context ƒë·∫ßy ƒë·ªß
        """
        results = await self.search(query, max_results=3)
        
        if not results:
            return ""
        
        context_parts = []
        current_chars = 0
        
        for result in results:
            if current_chars >= max_chars:
                break
            
            doc_id = result.metadata.get("doc_id", "")
            if doc_id and doc_id in self.documents:
                doc = self.documents[doc_id]
                content = doc.get("content", "")
                
                # L·∫•y n·ªôi dung ph√π h·ª£p
                remaining = max_chars - current_chars
                if len(content) > remaining:
                    content = content[:remaining] + "..."
                
                context_parts.append(f"üìÑ **{result.title}**\n{content}")
                current_chars += len(content)
        
        return "\n\n---\n\n".join(context_parts)

# ============================================================
# HYBRID RAG ENGINE
# ============================================================

class HybridRAGEngine:
    """
    Hybrid RAG Engine - K·∫øt h·ª£p Web Search v√† Local Knowledge Base
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or DEFAULT_RAG_CONFIG
        self.web_search = DuckDuckGoSearch(self.config.get("web_search"))
        self.knowledge_base = LocalKnowledgeBase(self.config.get("knowledge_base"))
        self._load_config()
    
    def _load_config(self):
        """Load config t·ª´ file"""
        try:
            if RAG_CONFIG_FILE.exists():
                with open(RAG_CONFIG_FILE, 'r', encoding='utf-8') as f:
                    saved_config = json.load(f)
                    self.config.update(saved_config)
        except Exception:
            pass
    
    def _save_config(self):
        """L∆∞u config v√†o file"""
        try:
            with open(RAG_CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
        except Exception:
            pass
    
    async def search(self, query: str, 
                     sources: List[str] = None,
                     max_results: int = 10) -> RAGContext:
        """
        T√¨m ki·∫øm hybrid t·ª´ c·∫£ web v√† local
        
        Args:
            query: C√¢u h·ªèi/t·ª´ kh√≥a
            sources: ["web", "local"] ho·∫∑c None cho c·∫£ hai
            max_results: S·ªë k·∫øt qu·∫£ t·ªïng c·ªông
            
        Returns:
            RAGContext: Context ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng
        """
        start_time = time.time()
        sources = sources or ["web", "local"]
        
        web_results = []
        local_results = []
        
        # T√¨m ki·∫øm song song
        tasks = []
        if "web" in sources:
            tasks.append(("web", self.web_search.search(query, max_results // 2 + 1)))
        if "local" in sources:
            tasks.append(("local", self.knowledge_base.search(query, max_results // 2 + 1)))
        
        # Ch·∫°y ƒë·ªìng th·ªùi
        for source_name, task in tasks:
            try:
                results = await task
                if source_name == "web":
                    web_results = results
                else:
                    local_results = results
            except Exception as e:
                print(f"‚ö†Ô∏è [RAG] {source_name} search error: {e}")
        
        # K·∫øt h·ª£p v√† x·∫øp h·∫°ng
        combined_results = self._rerank_results(web_results, local_results, query)
        
        # X√¢y d·ª±ng context
        context = self._build_context(query, combined_results[:max_results])
        
        search_time = (time.time() - start_time) * 1000
        
        return RAGContext(
            query=query,
            web_results=web_results,
            local_results=local_results,
            combined_context=context,
            sources=[r.url for r in combined_results[:max_results] if r.url],
            search_time_ms=search_time,
            timestamp=datetime.now().isoformat()
        )
    
    def _rerank_results(self, web_results: List[SearchResult], 
                        local_results: List[SearchResult],
                        query: str) -> List[SearchResult]:
        """
        X·∫øp h·∫°ng l·∫°i k·∫øt qu·∫£ v·ªõi weighted scoring
        """
        hybrid_config = self.config.get("hybrid", {})
        web_weight = hybrid_config.get("web_weight", 0.4)
        local_weight = hybrid_config.get("local_weight", 0.6)
        
        all_results = []
        
        # Normalize v√† apply weights
        for r in web_results:
            r.score = r.score * web_weight
            all_results.append(r)
        
        for r in local_results:
            r.score = r.score * local_weight
            all_results.append(r)
        
        # S·∫Øp x·∫øp theo score
        all_results.sort(key=lambda x: x.score, reverse=True)
        
        return all_results
    
    def _build_context(self, query: str, results: List[SearchResult]) -> str:
        """
        X√¢y d·ª±ng context string cho LLM
        """
        if not results:
            return ""
        
        context_parts = [
            f"üìä **Th√¥ng tin tra c·ª©u cho: \"{query}\"**",
            f"üïê Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            ""
        ]
        
        web_section = []
        local_section = []
        
        for r in results:
            entry = f"### {r.title}\n{r.snippet}"
            if r.url and not r.url.startswith("file://"):
                entry += f"\nüîó {r.url}"
            
            if r.source == "web":
                web_section.append(entry)
            else:
                local_section.append(entry)
        
        if web_section:
            context_parts.append("## üåê T·ª´ Internet (DuckDuckGo)")
            context_parts.extend(web_section)
            context_parts.append("")
        
        if local_section:
            context_parts.append("## üìö T·ª´ T√†i li·ªáu n·ªôi b·ªô")
            context_parts.extend(local_section)
        
        return "\n\n".join(context_parts)
    
    async def get_answer_context(self, query: str, 
                                  prefer_source: str = "auto") -> str:
        """
        L·∫•y context t·ªëi ∆∞u ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi
        
        Args:
            query: C√¢u h·ªèi
            prefer_source: "web", "local", ho·∫∑c "auto"
            
        Returns:
            str: Context s·∫µn s√†ng cho LLM
        """
        # Ph√¢n t√≠ch query ƒë·ªÉ ch·ªçn source ph√π h·ª£p
        if prefer_source == "auto":
            prefer_source = self._detect_best_source(query)
        
        if prefer_source == "local":
            # ∆Øu ti√™n local, fallback web
            local_context = await self.knowledge_base.get_full_context(query)
            if local_context:
                return f"üìö **Th√¥ng tin t·ª´ t√†i li·ªáu n·ªôi b·ªô:**\n\n{local_context}"
            # Fallback to web
            prefer_source = "web"
        
        if prefer_source == "web":
            rag_context = await self.search(query, sources=["web"], max_results=5)
            if rag_context.combined_context:
                return rag_context.combined_context
        
        # Hybrid search
        rag_context = await self.search(query, max_results=8)
        return rag_context.combined_context
    
    def _detect_best_source(self, query: str) -> str:
        """
        Ph√°t hi·ªán source ph√π h·ª£p nh·∫•t cho query
        """
        query_lower = query.lower()
        
        # Keywords g·ª£i √Ω local
        local_keywords = [
            't√†i li·ªáu', 'file', 'document', 'h·ª£p ƒë·ªìng', 'b√°o c√°o',
            'ghi ch√∫', 'notes', 'd·ª± √°n', 'project', 'n·ªôi b·ªô',
            'c·ªßa t√¥i', 'my', 'our', 'c√¥ng ty', 'company'
        ]
        
        # Keywords g·ª£i √Ω web
        web_keywords = [
            'tin t·ª©c', 'news', 'm·ªõi nh·∫•t', 'latest', 'h√¥m nay', 'today',
            'gi√°', 'price', 'th·ªùi ti·∫øt', 'weather', 't·ª∑ gi√°', 'exchange',
            'wiki', 'wikipedia', 'google', 'search', 'tra c·ª©u online'
        ]
        
        local_score = sum(1 for kw in local_keywords if kw in query_lower)
        web_score = sum(1 for kw in web_keywords if kw in query_lower)
        
        if local_score > web_score:
            return "local"
        elif web_score > local_score:
            return "web"
        else:
            return "hybrid"

# ============================================================
# RAG TOOLS FOR MCP
# ============================================================

# Global RAG Engine instance
_rag_engine: Optional[HybridRAGEngine] = None

def get_rag_engine() -> HybridRAGEngine:
    """Get or create RAG engine instance"""
    global _rag_engine
    if _rag_engine is None:
        _rag_engine = HybridRAGEngine()
    return _rag_engine

async def web_search(query: str, max_results: int = 5) -> dict:
    """
    üåê T√¨m ki·∫øm tr√™n Internet (DuckDuckGo)
    
    Args:
        query: T·ª´ kh√≥a t√¨m ki·∫øm
        max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa (m·∫∑c ƒë·ªãnh 5)
        
    Returns:
        dict: K·∫øt qu·∫£ t√¨m ki·∫øm
    """
    try:
        engine = get_rag_engine()
        results = await engine.web_search.search(query, max_results)
        
        if not results:
            return {
                "success": False,
                "message": "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£",
                "query": query,
                "results": []
            }
        
        # Th√™m th√¥ng tin ng√†y hi·ªán t·∫°i ƒë·ªÉ LLM d·ªÖ so s√°nh th·ªùi gian
        now = datetime.now()
        
        return {
            "success": True,
            "query": query,
            "count": len(results),
            "results": [
                {
                    "title": r.title,
                    "snippet": r.snippet,
                    "url": r.url,
                    "score": r.score
                }
                for r in results
            ],
            "timestamp": now.isoformat(),
            "current_date": now.strftime("%d/%m/%Y"),
            "analysis_hint": f"‚ö†Ô∏è H√¥m nay l√† {now.strftime('%d th√°ng %m nƒÉm %Y')}. Khi ph√¢n t√≠ch k·∫øt qu·∫£, n·∫øu b√†i vi·∫øt n√≥i 'd·ª± ki·∫øn' ho·∫∑c 's·∫Øp ra m·∫Øt' v√†o m·ªôt ng√†y ƒê√É QUA, nghƒ©a l√† s·ª± ki·ªán ƒë√≥ ƒê√É X·∫¢Y RA r·ªìi!"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "query": query
        }

async def rag_search(query: str, 
                     sources: str = "auto",
                     max_results: int = 8) -> dict:
    """
    üîç RAG Search - T√¨m ki·∫øm hybrid t·ª´ Internet + T√†i li·ªáu n·ªôi b·ªô
    
    Args:
        query: C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a
        sources: "web", "local", "auto", ho·∫∑c "hybrid" (m·∫∑c ƒë·ªãnh: auto)
        max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
        
    Returns:
        dict: Context v√† k·∫øt qu·∫£ t√¨m ki·∫øm
    """
    try:
        engine = get_rag_engine()
        
        # X√°c ƒë·ªãnh sources
        if sources == "auto":
            source_list = None  # Engine s·∫Ω t·ª± detect
        elif sources == "web":
            source_list = ["web"]
        elif sources == "local":
            source_list = ["local"]
        else:
            source_list = ["web", "local"]
        
        rag_context = await engine.search(query, source_list, max_results)
        
        return {
            "success": True,
            "query": query,
            "context": rag_context.combined_context,
            "web_count": len(rag_context.web_results),
            "local_count": len(rag_context.local_results),
            "sources": rag_context.sources,
            "search_time_ms": round(rag_context.search_time_ms, 2),
            "timestamp": rag_context.timestamp
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "query": query
        }

async def get_realtime_info(query: str) -> dict:
    """
    ‚ö° L·∫•y th√¥ng tin TH·ªúI GIAN TH·ª∞C t·ª´ Internet
    
    T·ª± ƒë·ªông tra c·ª©u DuckDuckGo ƒë·ªÉ l·∫•y th√¥ng tin m·ªõi nh·∫•t
    tr∆∞·ªõc khi tr·∫£ l·ªùi. D√πng cho: tin t·ª©c, gi√° c·∫£, th·ªùi ti·∫øt,
    t·ª∑ gi√°, s·ª± ki·ªán hi·ªán t·∫°i, v.v.
    
    üÜï CRYPTO PRICES: T·ª± ƒë·ªông d√πng API chuy√™n d·ª•ng cho gi√° crypto
    
    Args:
        query: C√¢u h·ªèi c·∫ßn th√¥ng tin th·ªùi gian th·ª±c
        
    Returns:
        dict: Th√¥ng tin ƒë√£ tra c·ª©u
    """
    try:
        # üÜï CHECK IF CRYPTO QUERY - Use dedicated API
        if CRYPTO_API_AVAILABLE:
            crypto_symbol = await detect_crypto_query(query)
            if crypto_symbol:
                print(f"üí∞ [CryptoAPI] Detected crypto query: {crypto_symbol}")
                
                # Try CoinGecko first
                crypto_data = await get_crypto_price(crypto_symbol)
                
                # Fallback to Binance
                if not crypto_data and crypto_symbol == "bitcoin":
                    crypto_data = await get_crypto_price_binance("BTCUSDT")
                
                if crypto_data:
                    # Format context with accurate data
                    crypto_context = f"""üìä **Th√¥ng tin {crypto_data.get('name', crypto_symbol.upper())} (Realtime t·ª´ {crypto_data['source']})**

üíµ **Gi√° hi·ªán t·∫°i**: ${crypto_data['price_usd']:,.2f} USD
üìà **Thay ƒë·ªïi 24h**: {crypto_data['price_change_24h']:+.2f}%
üíé **Gi√° cao nh·∫•t (ATH)**: ${crypto_data.get('ath', 0):,.2f} USD"""
                    
                    if 'ath_date' in crypto_data:
                        crypto_context += f" (ƒë·∫°t v√†o {crypto_data['ath_date']})"
                    
                    if 'market_cap' in crypto_data:
                        crypto_context += f"\nüìä **V·ªën h√≥a th·ªã tr∆∞·ªùng**: ${crypto_data['market_cap']:,.0f} USD"
                    
                    crypto_context += f"\nüïê **C·∫≠p nh·∫≠t**: {crypto_data['timestamp']}"
                    
                    return {
                        "success": True,
                        "query": query,
                        "realtime_context": crypto_context,
                        "sources": [crypto_data['source']],
                        "search_time_ms": 0,
                        "current_date": datetime.now().strftime("%d/%m/%Y"),
                        "data_type": "crypto_api",
                        "note": f"‚úÖ D·ªØ li·ªáu CH√çNH X√ÅC 100% t·ª´ {crypto_data['source']} API"
                    }
        
        # Default: Web search
        engine = get_rag_engine()
        
        # Lu√¥n ∆∞u ti√™n web cho realtime info
        rag_context = await engine.search(query, sources=["web"], max_results=5)
        
        if not rag_context.web_results:
            return {
                "success": False,
                "message": "Kh√¥ng th·ªÉ tra c·ª©u th√¥ng tin. Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng.",
                "query": query
            }
        
        # Th√™m th√¥ng tin ng√†y hi·ªán t·∫°i ƒë·ªÉ LLM d·ªÖ so s√°nh th·ªùi gian
        now = datetime.now()
        
        return {
            "success": True,
            "query": query,
            "realtime_context": rag_context.combined_context,
            "sources": rag_context.sources,
            "search_time_ms": round(rag_context.search_time_ms, 2),
            "current_date": now.strftime("%d/%m/%Y"),
            "analysis_hint": f"‚ö†Ô∏è NG√ÄY HI·ªÜN T·∫†I: {now.strftime('%d th√°ng %m nƒÉm %Y')}. Khi c√≥ b√†i vi·∫øt n√≥i 'd·ª± ki·∫øn ra m·∫Øt th√°ng X' m√† th√°ng X ƒê√É QUA ‚Üí s·ª± ki·ªán ƒê√É X·∫¢Y RA. H√£y d√πng th√¨ qu√° kh·ª© ho·∫∑c hi·ªán t·∫°i, KH√îNG n√≥i 'd·ª± ki·∫øn' cho s·ª± ki·ªán ƒë√£ qua!",
            "note": "‚ö° Th√¥ng tin ƒë∆∞·ª£c tra c·ª©u t·ª´ Internet ngay l·∫≠p t·ª©c"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "query": query
        }

async def smart_answer(query: str) -> dict:
    """
    üß† Smart Answer - T·ª± ƒë·ªông ch·ªçn ngu·ªìn ph√π h·ª£p nh·∫•t
    
    AI s·∫Ω ph√¢n t√≠ch c√¢u h·ªèi v√† quy·∫øt ƒë·ªãnh:
    - D√πng Knowledge Base n·ªôi b·ªô
    - Tra c·ª©u Internet
    - Ho·∫∑c k·∫øt h·ª£p c·∫£ hai
    
    Args:
        query: C√¢u h·ªèi c·ªßa user
        
    Returns:
        dict: Context t·ªëi ∆∞u cho c√¢u tr·∫£ l·ªùi
    """
    try:
        engine = get_rag_engine()
        
        # Detect best source
        best_source = engine._detect_best_source(query)
        
        # Get context
        context = await engine.get_answer_context(query, best_source)
        
        return {
            "success": True,
            "query": query,
            "detected_source": best_source,
            "context": context,
            "instruction": "S·ª≠ d·ª•ng context tr√™n ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu context kh√¥ng ƒë·ªß, h√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n v√† ghi ch√∫ r√µ.",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "query": query
        }

# ============================================================
# RAG TOOL DEFINITIONS FOR MCP
# ============================================================

RAG_TOOLS = {
    "web_search": {
        "handler": web_search,
        "description": "üåê T√åM KI·∫æM INTERNET (DuckDuckGo) - Tra c·ª©u th√¥ng tin m·ªõi nh·∫•t t·ª´ web. D√πng khi c·∫ßn: tin t·ª©c, gi√° c·∫£, th·ªùi ti·∫øt, s·ª± ki·ªán hi·ªán t·∫°i, th√¥ng tin v·ªÅ ng∆∞·ªùi/c√¥ng ty/s·∫£n ph·∫©m. VD: 'gi√° v√†ng h√¥m nay', 'tin t·ª©c c√¥ng ngh·ªá', 'th·ªùi ti·∫øt H√† N·ªôi'.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "T·ª´ kh√≥a t√¨m ki·∫øm"
                },
                "max_results": {
                    "type": "integer",
                    "description": "S·ªë k·∫øt qu·∫£ t·ªëi ƒëa (m·∫∑c ƒë·ªãnh 5)",
                    "default": 5
                }
            },
            "required": ["query"]
        }
    },
    "rag_search": {
        "handler": rag_search,
        "description": "üîç RAG SEARCH HYBRID - T√¨m ki·∫øm k·∫øt h·ª£p t·ª´ Internet + T√†i li·ªáu n·ªôi b·ªô. T·ª± ƒë·ªông ch·ªçn ngu·ªìn ph√π h·ª£p nh·∫•t. D√πng sources='web' cho Internet only, 'local' cho t√†i li·ªáu n·ªôi b·ªô only, 'hybrid' cho c·∫£ hai.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a t√¨m ki·∫øm"
                },
                "sources": {
                    "type": "string",
                    "description": "Ngu·ªìn: 'auto', 'web', 'local', 'hybrid'",
                    "default": "auto"
                },
                "max_results": {
                    "type": "integer",
                    "description": "S·ªë k·∫øt qu·∫£ t·ªëi ƒëa",
                    "default": 8
                }
            },
            "required": ["query"]
        }
    },
    "get_realtime_info": {
        "handler": get_realtime_info,
        "description": "‚ö° TH√îNG TIN TH·ªúI GIAN TH·ª∞C - Tra c·ª©u Internet NGAY L·∫¨P T·ª®C tr∆∞·ªõc khi tr·∫£ l·ªùi. ‚ö†Ô∏è B·∫ÆT BU·ªòC d√πng khi user h·ªèi v·ªÅ: tin t·ª©c, gi√° c·∫£, t·ª∑ gi√°, th·ªùi ti·∫øt, s·ª± ki·ªán ƒëang di·ªÖn ra, th√¥ng tin c·∫≠p nh·∫≠t. Kh√¥ng d√πng ki·∫øn th·ª©c c≈©, LU√îN tra c·ª©u m·ªõi!",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "C√¢u h·ªèi c·∫ßn th√¥ng tin th·ªùi gian th·ª±c"
                }
            },
            "required": ["query"]
        }
    },
    "smart_answer": {
        "handler": smart_answer,
        "description": "üß† SMART ANSWER - AI t·ª± ƒë·ªông ph√¢n t√≠ch v√† ch·ªçn ngu·ªìn t·ªët nh·∫•t (Internet/T√†i li·ªáu n·ªôi b·ªô/Hybrid) ƒë·ªÉ tr·∫£ l·ªùi. D√πng khi kh√¥ng ch·∫Øc ch·∫Øn ngu·ªìn n√†o ph√π h·ª£p. Tool n√†y s·∫Ω tr·∫£ v·ªÅ context ƒë√£ t·ªëi ∆∞u.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "C√¢u h·ªèi c·ªßa user"
                }
            },
            "required": ["query"]
        }
    }
}

# ============================================================
# MAIN / TEST
# ============================================================

if __name__ == "__main__":
    async def test():
        print("üß™ Testing RAG System...")
        
        # Test web search
        print("\n1. Testing Web Search...")
        result = await web_search("tin t·ª©c c√¥ng ngh·ªá h√¥m nay")
        print(f"   Results: {result.get('count', 0)} items")
        
        # Test RAG search
        print("\n2. Testing RAG Search...")
        result = await rag_search("d·ª± √°n ph·∫ßn m·ªÅm", sources="hybrid")
        print(f"   Web: {result.get('web_count', 0)}, Local: {result.get('local_count', 0)}")
        
        # Test realtime info
        print("\n3. Testing Realtime Info...")
        result = await get_realtime_info("gi√° v√†ng h√¥m nay")
        print(f"   Success: {result.get('success', False)}")
        
        print("\n‚úÖ RAG System test completed!")
    
    asyncio.run(test())
